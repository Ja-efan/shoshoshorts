# 0 개요

Zonos-v0.1은 20만 시간 이상의 다양한 다국어 음성으로 학습된 선도적인 오픈 가중치 기반 TTS(Text-to-Speech) 모델로서, 뛰어난 표현력과 품질을 제공하며 일부 상용 TTS 공급자들을 능가하기도 합니다.

이 모델은 텍스트 프롬프트와 화자 임베딩 또는 오디오 프리픽스를 입력받으면 매우 자연스러운 음성을 생성할 수 있으며, 몇 초 분량의 레퍼런스 클립만으로도 정확하게 음성을 복제할 수 있습니다. 또한 이 모델은 말하기 속도, 음조(피치) 변동, 오디오 품질, 행복/두려움/슬픔/분노와 같은 감정들을 정밀하게 제어할 수 있도록 설계되었습니다. 모델 출력은 기본적으로 44kHz로 생성됩니다.

##### 자세한 내용과 음성 샘플은 [여기](https://www.zyphra.com/post/beta-release-of-zonos-v0-1) 블로그를 확인하세요.

##### 또한 [playground.zyphra.com/audio](https://playground.zyphra.com/audio)에서 호스팅 버전을 사용해 보실 수 있습니다.

# 1. 정리

1. 한국어 번역 Zonos 모델 슈트

Zonos-v0.1 모델 슈트는 1.6B 파라미터를 갖춘 두 가지 모델(트랜스포머 모델과 SSM 하이브리드 모델)로 구성되며, 둘 다 Apache 2.0 라이선스로 공개됩니다. 우리의 슈트는 트랜스포머 모델과 SSM 하이브리드 모델로 이루어져 있는데, 이는 특히 TTS를 위해 공개된 최초의 SSM 모델이라는 점에서 의미가 있습니다. 이러한 이중 접근 방식을 통해 우리는 오디오 생성에서 이러한 아키텍처들 간의 성능 및 품질 트레이드오프를 면밀히 조사할 수 있었습니다.

Zonos-v0.1 모델들은 약 20만 시간 분량의 음성 데이터로 학습되었으며, 오디오북 내레이션 같은 중립 톤 음성부터 매우 표현적인 음성까지 폭넓은 스타일을 포함합니다. 데이터의 대부분은 영어이지만, 중국어, 일본어, 프랑스어, 스페인어, 독일어도 상당한 비중을 차지합니다. 이 외에도 여러 언어가 조금씩 포함되어 있으나, 이 언어들에 대한 모델의 성능은 견고하지 않을 수 있습니다.

Zonos는 텍스트 프롬프트와 화자 임베딩(또는 오디오 프리픽스)을 입력받아 매우 표현력 있고 자연스러운 음성을 생성할 수 있습니다. 또한 5~30초 분량의 음성 클립이 주어지면 고품질 보이스 클로닝도 가능합니다. 말하기 속도, 피치 표준편차, 오디오 품질, 그리고 슬픔/두려움/분노/행복/놀람 같은 감정 입력을 조건으로 사용할 수도 있습니다. Zonos는 기본적으로 44kHz로 음성을 출력합니다.

Zonos 모델 성능
우리는 고도로 최적화된 추론 엔진을 통해 Zonos API와 플레이그라운드를 운영하고 있으며, 빠른 초음(Time-To-First-Audio, TTFA) 지표를 달성합니다. 특히 하이브리드 모델은 Mamba2 기반 아키텍처 덕분에, 트랜스포머 모델에 비해 어텐션 블록 의존도가 더 낮아 대기 시간(latency)과 메모리 사용량이 줄어드는 등 효율적 성능 특성을 보여줍니다.

학습 및 아키텍처 세부사항
Zonos-v0.1 모델들은 텍스트와 오디오 토큰이 주어졌을 때 해당 오디오 토큰 시퀀스를 오토리그레시브(autoregressive)하게 예측하는 간단한 과제에 맞춰 학습되었습니다. 이 오디오 토큰은 원본 음성 파형에서 descript audio codec(DAC) 오토인코더를 통해 추출됩니다. DAC는 고비트레이트 방식의 오토인코더로, 더 높은 음질을 얻을 수 있는 반면, 더 복잡한 예측 문제를 야기하고 1초당 더 많은 토큰을 생성해야 한다는 비용이 있습니다.
텍스트 입력은 먼저 정규화 과정을 거친 뒤 eSpeak 음소화기를 통해 음소를 얻은 다음, 트랜스포머 혹은 하이브리드 백본에 입력해 오디오 토큰을 예측합니다. 모델은 또한 화자 임베딩을 받아 보이스 클로닝 기능을 수행하며, 말하기 속도·피치·샘플레이트·오디오 품질·감정 등 다양한 조건 입력도 받아 생성 음성을 유연하게 제어할 수 있습니다.

Zonos-v0.1은 두 단계로 학습되었습니다.

1단계(전체 학습의 약 70%): 텍스트 프리픽스와 화자 임베딩만 사용해 사전학습(pre-training)을 진행.
2단계(나머지 30%): 모델에 말하기 속도, 피치 등 여러 조건을 추가로 입력하고, 고품질 데이터의 비중을 약간 높여 훈련.
이 2단계 학습 접근법을 통해 모델의 강건성과 전반적인 품질이 향상되었음을 확인했습니다.
우리 모델은 현재 공개된 모델 중 가장 높은 품질 한계점(quality ceiling)을 갖는다고 생각하지만, 몇 가지 한계도 존재합니다. 예를 들어 완전한 오토리그레션 방식으로 인해 모델이 자유롭게 출력을 만들어내는 만큼 실수할 여지도 커집니다. 내부 테스트에서, 생성된 음성 시작부나 끝부분에 기침, 클릭, 웃음소리, 찍찍거림(squeaks), 거친 숨소리 등 잡음(artifact)이 비교 모델 대비 다소 많았습니다. 또한 불규칙 문장(out-of-distribution)에서 특정 단어를 건너뛰거나 반복하는 텍스트 정렬 오류가 나타날 수도 있었습니다.

고비트레이트 오토인코더를 사용하기 때문에 음성 품질은 극대화되지만, 추론은 더 느리고 비용이 많이 듭니다. 1초 분량의 음성을 생성하기 위해 9개의 코드북에 걸쳐 86개의 프레임, 즉 1초당 774개의 토큰을 생성해야 합니다. 우리는 Parler TTS에서 제안된 delay codebook 패턴, 멀티 토큰 예측, 임베딩 병합(embedding merging) 기법을 사용해 생성 속도를 개선하고 문맥을 확장합니다. 구체적으로, 토큰 임베딩 단계에서 DAC의 9개 코드북(=9개 토큰)에 해당하는 그룹을 하나의 임베딩으로 합친 뒤 트랜스포머와 하이브리드를 거칩니다. 마지막 레이어에서는 9개 언어 모델링 헤드를 통해 각 임베딩에서 9개 토큰을 예측합니다. 이 기법은 생성 품질의 손실을 최소화하면서도 성능을 크게 개선해줍니다. 실제로 RTX 4090에서 약 200~300ms의 수용 가능한 지연(latency) 및 1을 훨씬 초과하는 실시간 계수(RTF)를 달성하지만, 아직 더 최적화할 여지는 남아 있습니다.

향후 모델 릴리즈에서는 모델의 안정성, 특정 발음 처리 능력, 지원 언어 수 확대, 감정이나 음성 특성에 대한 제어 범위 개선 등을 목표로 개발을 이어갈 예정입니다. 또한 모델 품질과 추론 성능을 높이기 위한 추가적인 아키텍처 혁신도 추진할 계획입니다.

2. 주요 내용 요약 및 인사이트
   모델 구성

1.6B 파라미터를 갖춘 트랜스포머 모델과 SSM 하이브리드 모델 두 가지 버전을 공개 (Apache 2.0 라이선스).
공개된 TTS용 최초의 SSM 모델이라는 점에서 의미가 있음.
하이브리드 모델은 트랜스포머 대비 어텐션 부담을 줄여 지연(latency)와 메모리 사용량을 낮춤.
대규모 학습 데이터

20만 시간 이상의 음성 데이터로 학습.
영어가 주를 이루지만 중국어, 일본어, 프랑스어, 스페인어, 독일어에 대한 대규모 데이터도 포함.
이외 언어들도 소량 포함되지만, 그 언어들에 대한 성능은 제한적.
기능 및 특징

보이스 클로닝: 5~30초 음성 클립으로 고품질 복제 가능.
말하기 속도, 피치, 감정(슬픔, 두려움, 분노, 행복, 놀람 등) 조절 가능.
기본 44kHz 출력으로 고품질 음성 합성.
오토리그레시브 기반으로 자연스러운 발화 가능하지만, 드물게 음성 잡음이나 단어 누락/반복 발생.
DAC 기반 고비트레이트 오토인코더

더 높은 음질(quality ceiling)을 위해 DAC를 채택.
단, 1초당 774개의 토큰을 생성해야 해서 추론 비용과 시간이 높아지는 문제가 있음.
토큰 그룹화 및 멀티-토큰 예측으로 이를 일부 완화.
성능

최적화된 추론 엔진을 통해 빠른 Time-To-First-Audio(TTFA) 달성.
RTX 4090 기준, 실시간을 훨씬 웃도는 RTF(1보다 큼)와 200~300ms 정도의 지연을 달성.
향후 과제

더 많은 언어 지원, 특정 발음 처리 개선, 감정/음성 특성 제어 범위 확대.
아키텍처 혁신을 통해 모델 품질과 추론 성능 높이기.
오토리그레션 문제(잡음, 단어 반복/누락)도 추가 보완 계획.

Zonos는 간단한 구조를 따릅니다: eSpeak를 통해 텍스트 정규화와 음소화 과정을 거친 후, 트랜스포머(혹은 하이브리드) 백본을 이용해 DAC 토큰(디코더용 토큰)을 예측합니다. 전체 아키텍처 개요는 아래 다이어그램을 참고하세요.

## 2. 첨부된 이미지(아키텍처 다이어그램) 설명

이미지에는 대략 다음과 같은 흐름이 시각화되어 있습니다.

1. **Text Pipeline**

   - 텍스트 입력을 받아 **Text Norm**(텍스트 정규화) → **eSpeak NG**를 통한 음소화 → **IPA**(국제 음성 기호) 변환 과정을 거칩니다.
   - 변환된 음소들은 **Embedding Table**에서 임베딩 벡터로 매핑됩니다.

2. **Conditioning Inputs & Prefix Embeddings**

   - 화자 식별 정보(`Speaker ID`), 감정(`Emotion`), 피치 표준편차(`Pitch STD`) 등의 추가 입력도 각각 임베딩으로 변환된 뒤, 텍스트 임베딩과 함께 **Concatenate(병합)** 단계에서 결합됩니다.
   - 이후 “Shared Projection” 블록에서 여러 임베딩들을 일정한 차원으로 정규화(프로젝션)합니다.
   - 또한 “Prefix Embeddings”라는 모듈이 있어, 몇 초 분량의 음성을 인코딩하여 특정 발화 특징(예: 속삭임 등)을 반영하기 위한 오디오 프리픽스용 임베딩을 생성/수용합니다.

3. **Backbone (Transformer 혹은 Hybrid)**

   - 병합된 임베딩 시퀀스가 **Backbone**으로 들어갑니다.
   - Transformer 백본인 경우 “RoPE MHSA” (RoPE: Rotary Positional Embedding, MHSA: Multi-Head Self-Attention) 블록, “Feed Forward SwiGLU” 블록, Layer Norm 등이 반복(xN)됩니다.
   - Hybrid 백본인 경우 Transformer 블록 위에 “Mamba2 Block”이 여러 번 쌓여 있는 형태로, Transformer와 Mamba2 블록들을 혼합합니다.

4. **Delay Pattern Reversion/Building**

   - 음성 생성 과정에서 시간축 처리를 위해 “Revert Delay Pattern” 및 “Build Delay Pattern” 같은 모듈을 사용합니다. 이들은 모델 내부적으로 프레임 지연 구조 등을 재배열하여 음성 합성의 품질과 효율을 높이는 것으로 보입니다.

5. **출력(RVQ 계층)**
   - 모델은 최종적으로 오디오 코드를 생성하며, 여러 단계의 RVQ(Residual Vector Quantizer) 코드 출력을 통해 고음질의 음성 신호를 복원할 수 있게 됩니다.
   - 이 디코딩 과정(“Autoencoder”)을 거쳐 44kHz의 최종 음성을 얻게 됩니다.

이미지 전반적으로 **텍스트 + 여러 조건(화자, 감정, 피치 등) → 통합 임베딩 → Transformer/Hybrid 백본 → 시간 지연 패턴 처리 → 최종 음성 코드 → 디코더** 과정을 한눈에 확인할 수 있습니다.

## 3. 모델 구조 및 성능 위주의 인사이트

1. **방대한 학습 데이터(200k+ 시간)**

   - 20만 시간 이상의 다양한 언어와 스피커 데이터로 학습되어, 다국어 지원뿐 아니라 여러 음색·억양에서도 비교적 자연스러운 합성이 가능한 것으로 보입니다.
   - 대규모 데이터로 인해 **제로샷(Zero-Shot) 음성 합성**이나 **보이스 클로닝**이 정확하고 유연하게 동작합니다.

2. **하이브리드(Transformer + Mamba2) vs 순수 Transformer**

   - 모델이 두 가지 백본 버전을 제공합니다.
   - 순수 Transformer 버전은 구조가 단순하면서도 보편적으로 잘 알려진 메커니즘을 사용하여, 호환성과 안정성이 높은 편입니다.
   - 하이브리드 버전은 Transformer 블록을 기반으로 Mamba2 블록을 추가로 쌓아 좀 더 발전된 표현력을 추구합니다. 다만 3000 시리즈 이상의 GPU가 필요하므로, 환경 제약에 따라 모델 선택을 고려해야 합니다.

3. **세분화된 조건(Conditioning) 제어**

   - 화자 임베딩, 오디오 프리픽스, 감정 임베딩, 피치 컨트롤 등 다양한 측면에서 음성 신호를 제어할 수 있어 커스터마이징이 폭넓습니다.
   - 특히 ‘오디오 프리픽스’를 통해 예시 음성 특성을 세밀하게 반영할 수 있으므로, 단순 화자 임베딩 이상의 정교한 음성합성이 가능합니다(속삭임, 특수한 발성 등).

4. **실시간 처리 성능**

   - RTX 4090 기준 실시간의 2배 속도(RTF 0.5) 정도로 상당히 빠른 편이며, 이는 상호작용이나 서비스 배포 관점에서 큰 장점입니다.

5. **폭넓은 언어 및 감정 표현력**

   - 영어, 일본어, 중국어, 프랑스어, 독일어를 다룰 수 있고, 감정 범위도 다양하게 지원합니다.
   - TTS 시스템에서 감정 표현은 고급 기능에 속하기 때문에, 대화형 혹은 스토리텔링 시나리오에 사용 가치가 높습니다.

6. **확장 가능성과 오픈 가중치**
   - 오픈소스 형태로 배포되므로, 연구나 커스터마이징을 원하는 사용자 입장에서는 큰 유연성을 가집니다.
   - DAC 토큰 방식과 RVQ 기반 디코딩 구조가 정리되어 있어, 다른 목적지향 음성합성(task-specific TTS)에도 재활용하기 유리할 것으로 보입니다.

종합적으로 볼 때 **Zonos**는 대규모 데이터와 다양한 조건(감정, 화자, 음성특징 등)에 대응하는 탄탄한 구조를 갖춘 TTS 모델입니다. 하이브리드 아키텍처의 추가 블록(Mamba2)은 Transformer의 한계를 보완하거나 음질/표현력을 높이는 역할을 하며, 실제로 제공되는 속도와 품질 또한 상업적 수준에 가까워 보입니다. 특히 1) 간단한 API, 2) 다국어와 감정/화자 제어, 3) 오디오 프리픽스 기능 등이 현행 TTS 요구사항을 폭넓게 충족시켜 준다는 점이 인상적입니다.

# 3. 용어 정리

## 1. 텍스트 프리픽스 (Text Prefix)

텍스트 프리픽스는 말 그대로 **텍스트 입력 앞부분에 붙는 텍스트 조각**을 뜻합니다.

왜 필요하나요?

예를 들어, GPT 같은 언어 모델에 문장을 생성시키고 싶을 때, 먼저 예시 문장(“안녕하세요, 저는…”)을 입력해두면 그 분위기나 스타일을 이어받아 결과를 생성하죠. 이러한 처음에 주어지는 예시 문장을 **프리픽스(prefix)**라고 합니다.
TTS(Text-to-Speech) 관점에서도, 모델이 음성을 합성할 때 특정 맥락이나 스타일을 유지하도록 “예시 텍스트”를 미리 넣어두는 역할을 할 수 있습니다.
간단 예시
“사과가 빨갛다.”라는 텍스트를 TTS에 넣어 음성을 생성한다고 칩시다. 만약 특정 말투나 톤을 유도하고 싶다면, 그 말투가 담긴 문장 몇 개를 “프리픽스”로 먼저 제시하고, 이어서 본문 텍스트(“사과가 빨갛다.”)를 입력해 모델에게 연결된 맥락을 주는 방식입니다.

## 2. 화자 임베딩 (Speaker Embedding)

무엇인가요?
“임베딩(embedding)”이란 어떤 대상(여기서는 ‘화자(사람 목소리)’의 특징)을 숫자 벡터로 바꾼 것입니다. 화자 임베딩은 특정 사람의 목소리 특징을 압축해서 표현하는 일종의 ID이자, 목소리 “지문” 같은 역할을 합니다.
왜 필요하나요?
TTS에서 화자 임베딩을 입력해주면, 모델이 “아, 이 사람 목소리는 이런 톤/음역대/음색을 가지고 있구나”라고 이해하고, 그 화자의 목소리를 흉내 내거나 합성할 수 있습니다.
간단 예시
10초 정도 “김영희”라는 사람이 말하는 음성을 녹음해서 모델에 넣으면, 모델 내부에서는 그 목소리를 대표하는 숫자 벡터(화자 임베딩)를 생성합니다. 이후 “안녕하세요”라고 텍스트만 줘도, “김영희” 목소리를 흉내 내는 음성을 합성하게 됩니다.

## 3. Quality Ceiling (품질 한계)

무엇인가요?
Quality Ceiling은 모델이 이론적으로 낼 수 있는 최대 품질을 가리킵니다. 마치 천장처럼, 모델의 성능이 올라갈 수 있는 최대치(한계)라고 볼 수 있습니다.
왜 중요하나요?
어떤 모델이든 기술적으로 개선할 수 있는 여지는 있지만, 구조나 학습 데이터 등에 의해 한계가 정해지기도 합니다. quality ceiling이 높을수록, 이론적으로 더 자연스럽고 인간에 가까운 음성 합성이 가능하다고 여겨집니다.
간단 예시
CD 음질(44.1kHz)로 인코딩된 음악과, 라디오 AM 음질을 생각해보면, 최대한 같은 곡을 잘 녹음/재생해도 AM 라디오는 품질이 선명하지 못하죠. 이는 음성 코덱이나 모델 구조가 허용하는 품질의 상한선이 다르기 때문입니다.

## 4. DAC (Descript Audio Codec)

무엇인가요?
DAC는 “Descript Audio Codec”의 약자로, 원본 음성을 신경망 방식으로 압축(인코딩)하고, 다시 복원(디코딩)할 수 있게 해주는 고비트레이트 오디오 코덱입니다.
TTS에서 종종 “오토인코더(autoencoder)” 방식으로 쓰이는데, 음성을 여러 개의 “코드북(codebook)” 토큰으로 쪼개어 표현합니다.
왜 사용하나요?
음성을 직접 샘플 단위(1초에 수만 개 샘플)로 예측하는 것은 계산량이 엄청납니다. DAC 같은 코덱을 쓰면 훨씬 적은 “토큰(압축된 단위)”만 예측하면 되므로, 학습과 합성이 더 효율적일 수 있습니다.
하지만 DAC의 “고비트레이트”는 여전히 1초 생성에 많은 토큰을 필요로 하므로, 더 정교한 음질을 얻는 대신 계산 비용도 커지는 트레이드오프가 생깁니다.
간단 예시
CD 음질의 음성 신호를 그대로 예측하려면 1초에 44,100번 샘플을 모델이 전부 예측해야 할 것입니다. 하지만 DAC를 쓰면 1초 음성을 ‘몇백 개’ 정도의 토큰으로 표현해 예측량을 대폭 줄일 수 있습니다(물론 “몇백 개”도 적은 건 아니지만, 샘플 단위에 비하면 훨씬 작은 편입니다).

## 5. 오디오 프리픽스 (Audio Prefix)

오디오 프리픽스는 **“짧은 길이의 실제 음성 신호를 모델 앞부분에 넣어서, 그 음성 스타일을 이어받게 하는 것”**을 뜻합니다.
화자 임베딩과 비슷해 보이지만, 화자 임베딩이 “한 사람의 전체적인 목소리 특징”이라면, 오디오 프리픽스는 “특정 상황이나 특정 발성 스타일”을 좀 더 생생하게 전달합니다.
왜 필요하나요?
화자 임베딩만으로는 잡아내기 어려운 특수 상황(예: 속삭이는 톤, 특정 감정이 섞인 말투 등)을 모델에게 “이렇게 말해야 해!”라고 직접 예시를 주는 방식입니다.
간단 예시
어떤 사람이 속삭이듯 말하는 3초짜리 음성을 모델에 넣고, 이어서 텍스트 입력(“비밀번호를 알려드릴게요…”)을 하면, 모델은 “아, 지금은 속삭이는 목소리로 말하라는 거구나!”를 인식해 결과 음성도 비슷한 분위기로 생성하게 됩니다.

## 6. Delay Codebook 패턴

1. 어떤 문제를 해결하려고 하나요?
   최신 TTS 모델들은 오디오 코덱(예: DAC)을 이용해 음성을 여러 “토큰(token)”으로 쪼개서 예측합니다.
   예를 들어 1초 음성을 9개의 “코드북(codebook)”에 걸쳐 예측해야 한다면, 한 시점당 9개 코드를 모두 한 번에 예측할 수도 있지만, 이를 전부 동시에 혹은 순서대로 예측하면 메모리나 계산 효율 측면에서 부담이 커집니다.
2. Delay Codebook가 무엇인가요?
   “Delay Codebook”는 각 코드북의 예측을 살짝씩 지연시켜(혹은 순차적으로 겹치게 하여) 모델이 더 효율적으로 토큰을 처리할 수 있도록 하는 기법입니다.
   즉, 모든 코드북 토큰을 동시 예측하기보다는, 특정 시점에서 일부 코드북 토큰만 먼저 예측하고, 이후 단계에서 나머지를 예측하는 식으로 예측을 분산시킵니다.
3. 왜 유용한가요?
   모든 코드를 한꺼번에 처리하지 않고 단계적으로 나누어 예측하므로, 모델이 필요한 부분만 먼저 보고, 이후 단계에서 추가 정보를 결합할 수 있습니다.
   이로 인해 메모리 사용량 감소, 추론 속도 개선, 오류가 누적되지 않도록 제어하는 등의 효과를 기대할 수 있습니다.

## 7. 멀티 토큰 예측 (Multi-Token Prediction)

1. 기본 배경
   전통적인 오토리그레시브(autoregressive) 접근에서는 매 순간 토큰을 하나씩 예측합니다.
   하지만 음성을 1초에 수백~수천 개 토큰으로 나눠야 하는 경우, 토큰 하나하나를 순서대로 예측하면 너무 느리고 비효율적입니다.
2. 멀티 토큰 예측이란?
   한 번에 여러 토큰을 동시에 예측하는 방법입니다. 예를 들어, 한 시점(time step)에서 9개 토큰을 동시에 추론해버리는 식입니다.
3. 왜 빠를까요?
   토큰을 하나씩 순차적으로 예측할 때는 “토큰 1 → 토큰 2 → 토큰 3 → …” 이라는 방식으로 많은 연산이 중복됩니다.
   반면 멀티 토큰 예측을 사용하면 예측 스텝 수가 크게 줄어들어 추론 시간을 단축시킬 수 있습니다.

## 8. 임베딩 병합 (Embedding Merging)

1. 어떤 맥락에서 쓰이나요?
   DAC처럼 “음성을 여러 토큰으로 쪼개서 모델 입력”으로 넣는 상황에서, 모델 입장에서는 입력 시퀀스가 매우 길어지기 쉽습니다. (1초를 774개 토큰으로 나누었다면, 10초는 7,740개 토큰!)
2. 임베딩 병합(embedding merging)이란?
   일정 개수의 토큰을 하나의 ‘묶음’으로 만들어, 이 묶음을 하나의 임베딩 벡터로 만드는 기법입니다.
   예를 들어, DAC에서 9개 코드북에 해당하는 9개 토큰을 **“한 덩어리”**로 묶고, 그 9개를 합쳐서 하나의 임베딩으로 표현합니다. 이후 모델(트랜스포머나 SSM 하이브리드)에는 “(9토큰)1 묶음, (9토큰)2 묶음, …” 식으로 줄어든 길이의 시퀀스를 입력하게 됩니다.
3. 장점
   모델이 처리해야 할 시퀀스 길이가 단축됩니다. (774개 → 86묶음 등으로 크게 감소)
   이로 인해 연산량, 메모리 사용량이 훨씬 줄어들고, 추론 속도도 빨라집니다.
   마지막에 예측을 할 때는, 각 묶음에서 다시 여러 개의 토큰(예: 9개)을 복원(prediction head)을 통해 동시에 뽑아내므로, 정확도/품질 손실이 최소화됩니다.
